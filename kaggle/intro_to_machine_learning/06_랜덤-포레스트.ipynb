{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 인트로\n","\n","의사 결정 트리는 당신에게 어려운 결정을 남깁니다. 잎이 많은 깊은 트리는 몇 채의 주택만으로 예측을 하기에 과적합됩니다. 그러나 잎이 적은 얕은 트리는 데이터에서 그만큼의 차이를 포착하지 못하기 때문에 성능이 떨어집니다.\n","\n","오늘날의 가장 정교한 모델링 기법도 과소적합과 과대적합의 문제를 겪습니다. 그러나, 많은 모델들은 더 나은 성능으로 이어질 수 있는 기발한 아이디어를 가집니다. 예를 들어 **랜덤 포레스트**를 살펴보겠습니다.\n","\n","랜덤 포레스트는 많은 트리를 사용하며, 각 트리의 예측값을 평균하여 최종 예측을 합니다. 일반적으로 단일 의사 결정 트리보다 예측 정확도가 훨씬 뛰어나며 기본 매개 변수에 대해서도 잘 작동합니다. 모델링을 계속 하면 훨씬 나은 성능을 가지는 모델들을 배우게 되겠지만, 이러한 모델들은 매개변수에 민감합니다.\n","\n","# 예제\n","\n","데이터를 불러오는 코드는 여러 번 봤습니다. 데이터를 불러온 후에는 다음 변수들이 있습니다.\n","- train_X\n","- val_X\n","- train_y\n","- val_y"]},{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"outputs":[],"source":["import pandas as pd\n","    \n","# 데이터를 불러옵니다\n","melbourne_file_path = '../../data/melbourne-housing-snapshot/melb_data.csv'\n","melbourne_data = pd.read_csv(melbourne_file_path) \n","# 결측값을 가지는 행을 필터링합니다\n","melbourne_data = melbourne_data.dropna(axis=0)\n","# 목표값과 특성을 선택합니다\n","y = melbourne_data.Price\n","melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', \n","                        'YearBuilt', 'Lattitude', 'Longtitude']\n","X = melbourne_data[melbourne_features]\n","\n","from sklearn.model_selection import train_test_split\n","\n","# 특성과 목표 데이터 모두를 훈련 및 검증 데이터로 나눕니다\n","# 분리는 난수 생성기에 기반합니다\n","# random_state를 특정지으면 언제나 같은 묶음으로 나뉨을 보장할 수 있습니다\n","train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)"]},{"cell_type":"markdown","metadata":{},"source":["Scikit-learn을 이용해 의사 결정 트리를 만든 방법과 유사하게 랜덤 포레스트 모델을 구축합니다 - `DecisionTreeRegressor` 대신 `RandomForestRegressor` 클래스를 사용합니다."]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":true},"outputs":[{"name":"stdout","output_type":"stream","text":["191669.7536453626\n"]}],"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_absolute_error\n","\n","forest_model = RandomForestRegressor(random_state=1)\n","forest_model.fit(train_X, train_y)\n","melb_preds = forest_model.predict(val_X)\n","print(mean_absolute_error(val_y, melb_preds))"]},{"cell_type":"markdown","metadata":{},"source":["# 결론\n","추가 개선의 여지가 있어 보이지만, 이는 가장 좋은 성능을 보인 의사 결정 트리의 오차 250,000에 비하면 큰 개선입니다. 단일 의사 결정 트리의 최대 깊이를 변경한 것처럼 랜덤 포레스트의 성능을 크게 변화시키는 매개변수가 있습니다. 그러나 랜덤 포레스트의 가장 큰 특징 중 하나는 이러한 조정 없이도 일반적으로 합리적으로 작동한다는 것입니다."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":2}
